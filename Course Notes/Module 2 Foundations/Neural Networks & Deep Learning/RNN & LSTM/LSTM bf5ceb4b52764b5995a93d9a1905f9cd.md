# LSTM

---

# What are they?

- Long Short-Term Memory networks (LSTMs) are a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem in traditional RNNs
    - The vanishing gradient problem arises when training RNNs on long sequences, as the gradients that are backpropagated through time tend to become very small, making it challenging for the network to learn dependencies over long-term sequences
- Replaces internal parts of an RNN encoder with a specialized module called a ***memory cell***

# Key Components

- **Memory Cells:**
    - LSTMs introduce the concept of memory cells, which are units that can store information for long durations
        - These cells have a self-contained memory state that can be updated or cleared based on the input data and the network's current state
- **Gates:**
    - LSTMs use three types of gates to control the flow of information:
        - **Input Gate:** Determines how much of the new information should be stored in the memory cell
        - **Forget Gate:** Decides what information from the current memory cell should be discarded
        - **Output Gate:** Determines the final output based on the current memory cell content
        - Notes:
            - These gates regulate the information entering, exiting, and staying in the memory cell
- **Cell State:**
    - The cell state runs through the entire sequence, allowing information to persist or be modified over time
        - The gates enable the LSTM to selectively update and forget information in the memory cell

# How do they work?

- Simple recurrent neural networks have ***long-term memory (cell state)*** in the form of weights
    - The weights change slowly during training, encoding general knowledge about the data
- They also have ***short-term memory (hidden state)*** in the form of ephemeral activations, which pass from each node to successive nodes
- The ***LSTM model introduces an intermediate type of storage via the memory cell***
    - A memory cell is a composite unit, built from simpler nodes in a specific connectivity pattern, with the novel inclusion of multiplicative nodes

## Gated Memory Cell (Cell State)

- Each memory cell is equipped with an *internal state* and a number of multiplicative gates that determine whether:
    1. A given input should impact the internal state (***the input gate***)
    2. The internal state should be flushed to 0 (**the *forget gate***)
    3. The internal state of a given neuron should be allowed to impact the cell’s output (***the output gate***)

## Gated Hidden State

- ***The key distinction between vanilla RNNs and LSTMs is that the latter support gating of the hidden state***
- This means that we have ***dedicated mechanisms for when a hidden state should be updated and also for when it should be reset***
- These mechanisms are learned and they address the concerns listed above
    - For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation
    - Likewise, we will learn to skip irrelevant temporary observations
    - Last, we will learn to reset the latent state whenever needed

## Input Gate, Forget Gate and Output Gate

- **Inputs:** Input at the current time step $x_t$ and the hidden state of the previous time step $H_{t-1}$
- **How Do they Interact?** Three fully connected layers with sigmoid activation functions compute the values of the input, forget, and output gates
    - As a result of the sigmoid activation, all values of the three gates are in the range of $[0,1]$
    - Additionally, we require an ***input node***, typically computed with a ***tanh*** activation function
        - Intuitively, the ***input gate*** determines how much of the input node’s value should be added to the current memory cell internal state
        - The ***forget gate*** determines whether to keep the current value of the memory or flush it
        - The ***output gate*** determines whether the memory cell should influence the output at the current time step

# Architecture

---

- All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer
    
    ![Untitled](LSTM%20bf5ceb4b52764b5995a93d9a1905f9cd/Untitled.png)
    

- LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way
    
    ![Untitled](LSTM%20bf5ceb4b52764b5995a93d9a1905f9cd/Untitled%201.png)
    

## Core Idea

- The key to LSTMs is the cell state, the horizontal line running through the top of the diagram
    - The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged
        
        ![Untitled](LSTM%20bf5ceb4b52764b5995a93d9a1905f9cd/Untitled%202.png)
        

- The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates
    - Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation
- The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through
    - ***A value of zero means*** “let nothing through,” while ***a value of one means*** “let everything through”
    - An LSTM has three of these gates, to protect and control the cell state

![Untitled](LSTM%20bf5ceb4b52764b5995a93d9a1905f9cd/Untitled%203.png)

# Memory Cell Components

## Forget Gate

- The first step in our LSTM is to decide what information we’re going to throw away from the cell state
    - This decision is made by a sigmoid layer called the “***forget gate layer***”
    - It looks at $*h_{t−1}*$ and $*x_t*$, and outputs a number between $0$ and $1$ for each number in the cell state $*C_{t−1}*$
        - 1 represents “completely keep this” while a 0 represents “completely get rid of this

$$
f_t = \sigma (X_t W_{x,f} + H_{t-1} W_{h,f} + b_{x,f} + b_{h,f})
$$

## Input Gate

- The next step is to decide what new information we’re going to store in the cell state
    - This has two parts:
        1. First, a sigmoid layer called the “input gate layer” decides which values we’ll update
            
            $$
            I_t = \sigma (X_t W_{x,i} + H_{t-1} W_{h,i} + b_{x,i} + b_{h,i})
            $$
            
        2. Next, a $tanh$ layer creates a vector of new candidate values, $\hat C_t$, that could be added to the state
        
        $$
        \hat C_t = \tanh(X_t W_{x,c} + H_{t-1} W_{h,c} + b_{x,c} + b_{h,c}
        $$
        
        - Note that using $\tanh$ provides a value range of $[-1,1]$
        - Note that this is sometimes referred to as $g_t$
- It’s now time to update the old cell state, $*C_{t−1}*$, into the new cell state $*C_t*$. The previous steps already decided what to do, we just need to actually do it
    - We multiply the old state by $*f_t*$, forgetting the things we decided to forget earlier
    - Then we add $*i_t* \hat C_t*$
        - This are the new candidate values, scaled by how much we decided to update each state value
    
    $$
    C_t = f_t * C_{t-1} + i_t * \hat C_{t-1}
    $$
    

## Output Gate

- Finally, we need to decide what we’re going to output
- This output will be based on our cell state, but will be a filtered version
    - First, we run a sigmoid layer which decides what parts of the cell state we’re  going to output
        
        $$
        O_t = \sigma (X_t W_{x,o} + H_{t-1} W_{h,o} + b_{x,o} + b_{h,o})
        $$
        
    - Last, we need to define how to compute the output of the memory cell, i.e., the hidden state as seen by other layers
        - We put the cell state through $\tanh$ (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to
        
        $$
        H_t = O_t \cdot \tanh(C_t)
        $$
        
        - Whenever the output gate is close to 1, we allow the memory cell internal state to impact the subsequent layers uninhibited, whereas for output gate values close to 0, we prevent the current memory from impacting other layers of the network at the current time step
            - Note that a memory cell can accrue information across many time steps without impacting the rest of the network (as long as the output gate takes values close to 0)

# Strengths & Limitations

## Strengths

- **Training Stability:**
    - LSTMs help address the vanishing gradient problem by providing a more stable training environment
        - The gating mechanisms allow the model to decide when to retain or discard information, reducing the impact of vanishing or exploding gradients during backpropagation
- **Handling Long-Term Dependencies:**
    - LSTMs are particularly effective at capturing long-term dependencies in sequential data
    - The memory cells, combined with the gating mechanisms, enable the network to learn and remember information over extended periods, making them suitable for tasks involving sequences, such as natural language processing, speech recognition, and time series prediction

# Resources

- [Dive into DL](https://d2l.ai/chapter_recurrent-modern/lstm.html)
- [Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)
- [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)
- [https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)
- [PyTorch Walkthrough](https://theaisummer.com/understanding-lstm/)

### Blogs

- [Intro to LSTM](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/)
- [https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c)
- [https://archive.ph/SOe2D](https://archive.ph/SOe2D)
- [Illustrated Guide to LSTM’s and GRU’s](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
- [https://cnvrg.io/pytorch-lstm/](https://cnvrg.io/pytorch-lstm/)

### Implementation

- [Encoder & Decoder LSTM with Attention Mechanism](https://medium.com/@eugenesh4work/attention-mechanism-for-lstm-used-in-a-sequence-to-sequence-task-be1d54919876)

### RNN Vs. LSTM

- [https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn](https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn)