# Softmax

# Overview

- The softmax activation function is a mathematical operation applied to a vector of real numbers, typically used in the output layer of a neural network for multi-class classification problems
- ***It converts raw scores or logits into a probability distribution over multiple classes***
    - The function ensures that the resulting probabilities sum to 1, making it suitable for representing the likelihood of an input belonging to each class
- Softmax operates like the sigmoid logistic function, but on a vector of values

# Definition

- The softmax function is used in the output layer to convert raw scores (logits) into a probability distribution over multiple classes. It is defined as:

$$
\text{softmax}(z_i) = \dfrac{e^{z_i}}{\sum_{j=1}^C e^{z_j}} 
$$

- where:
    - $z$ is the vector of raw scores for each class
    - $C$ is the total number of classes
    - $e$ is eulers number
    - The denominator is the sum of the exponentials of all logits in the vector
        - I.e normalization vector
    

# How It Works

1. **Normalization:**
    - The softmax function normalizes the raw scores by exponentiating each score and dividing it by the sum of exponentiated scores
        - This normalization ensures that the resulting values lie between 0 and 1
2. **Probability Distribution:**
    - The output of the softmax function can be interpreted as a probability distribution over the classes
        - Each element in the output vector represents the probability of the input belonging to the corresponding class
3. **Sum to 1:**
    - Due to the normalization step, the probabilities produced by the softmax function always sum to 1
        - This property is essential for the interpretation of the output as probabilities
4. **Amplifying Differences:**
    - The exponentiation in the softmax function has the effect of amplifying differences between the input scores. Larger scores get amplified more than smaller ones, making the model more confident in its predictions

# Why It Works

1. **Probability Interpretation:**
    - The softmax function transforms the raw scores into probabilities, providing a clear interpretation of the model's output in terms of class likelihoods
2. **Differentiability:**
    - The softmax function is differentiable, making it suitable for training neural networks using gradient-based optimization algorithms like backpropagation
3. **Cross-Entropy Loss:**
    - The softmax activation function is often paired with the cross-entropy loss, resulting in an objective function that is well-suited for training classification models
        - The cross-entropy loss measures the dissimilarity between predicted and true probability distributions
4. **Stabilizing Numerical Computations:**
    - Exponentiate to make big numbers really big and small numbers really small
    - The softmax function, by normalizing through exponentiation, helps stabilize the computation of probabilities, especially when dealing with large datasets
    

![Untitled](Softmax%2049699e07130243b4b4f4f6bf24da17b2/Untitled.png)

# Resources

- [https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78](https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78)
- [https://www.pinecone.io/learn/softmax-activation/](https://www.pinecone.io/learn/softmax-activation/)